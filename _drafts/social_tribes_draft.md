When I was reflecting on the work I did on Cornell Data Science this past year, I thought it might be interesting to look back even further and write a post about the first project I did for CDS. I feel like doing a post-mortem (or to put it less morbidly, debriefing) for the project could yield some interesting lessons both on the technical side and on the teamwork/organization side.

As a bit of background, this project was done during the first semester that my subteam on CDS existed. With my only machine learning background being from a single upper-div course, I wouldn't have been a good fit for CDS normally. But this subteam had a heavier emphasis on interpretability (of both methods and results) and I was brought onboard because of my experience with data visualization. I had expected to be mentored by more experienced members on the team, but due to the newness of the subteam, I ended up being one of the older and more experienced members.

The project was called "Social Tribes", and as expected of a proposal made by a freshman, its goal was far too broad and ambitious: "to detect and visualize echo chambers in social media". Needless to say, the scope was soon reduced to "clustering political journalists on Twitter". The team was myself, three freshmen, and one senior who had some experience with data visualization. 

We ended up scraping lists of Twitter followers from some 400 political journalists and creating a pairwise similarity metric based on number of overlapping followers which we hoped would represent the overlap in their audiences. Then, we applied some unsupervised clustering algorithms to the data and created some visualizations based on the projections. The cluster quality was quite poor and the results were mostly inconclusive, although the clusters we did find seemed to be split more along the lines of famous commentators vs lesser known journalists. 

The biggest technical mistakes we made were probably in feature extraction. Since some accounts had millions of followers and some had hundreds, we were only able to extract a small percentage of the followers of larger accounts but still retrieved the entire lists of smaller accounts (due to Twitter rate limits). This means that small differences in which followers were selected from large accounts may have a major impact on the similarity metric, which raises the possibility that the clustering we saw was entirely artificially created by bad feature extraction. Due to lack of statistics background among the members of the team, there was also some resistance against various statistical tests and cluster quality evaluations that could have given us more confidence in our results.

Looking back at our combined technical background, our odds of succeeding were not good; most of the freshmen had no experience with ML, and the senior ended up contributing nothing from the technical side. For my part, I tried my best to read textbooks and literature for modeling and visualization techniques we could use for the project, but implementing meaningful things was challenging because of our combined inexperience with Python ML frameworks. Even contributing to the same codebase was difficult, because most of the members knew neither Python nor Javascript. Despite making a conscious effort to make my code clean, readable, and commented, no other members made major contributions to the visualizations I wrote; although perhaps it was too much to expect since the visualizations were written in D3.js, which is somewhat difficult to pick up without significant onboarding/training. Although there had been some onboarding, it wasn't as comprehensive as later semesters since the team was new, and none of the other members were able to successfully pick up the framework during the course of the semester - this was one of the reasons why I created a new onboarding curriculum for the following semester. Since then, it has been refined further by other members and the current system (where freshmen do onboarding assignments in groups and are assigned a mentor) seems to be pretty effective.

To recover from my brief digression into onboarding, let's get back to talking about the project. From the start, this project suffered from a lack of leadership. There were three potential managers for this project: the senior (because he was the oldest), myself (because I had the most experience), and the freshman who created the project proposal. It was unreasonable to expect a freshman to effectively lead a team, but I was also reluctant to take charge because I didn't want to step on people's toes and be accused of hijacking the project. In the end, no one filled that role, and I believe that this negatively impacted the team's productivity and sense of direction. I sometimes regret not stepping in and taking charge of the project - as a sophomore I wasn't particularly confident in my ability to delegate tasks, but I guess it would have been better than nothing. I now know that a successful technical project needs an effective project manager, and that teams of more than 4 people probably cannot operate with everyone as an equal and volunteering for tasks instead of having them assigned. This is what motivated me to take more of a leadership role in thhe project I worked on the following semester, although that wasn't entirely successful either (more on that in a different post).

Overall, I think this project is pretty representative of the growing pains of creating a new team. It's always difficult to build up a knowledge base from scratch, and recruitment needs to be balanced between recruiting older, more experienced people who can contribute more v.s. recruiting younger, less experienced people who can continue the team in the future. I believe that effective onboarding and efficient project management are two key factors that can speed up this process, allowing a team to establish and distinguish itself much faster.

![boom photo]()

If nothing else, the Insights subteam on Cornell Data Science is good at coming up with exciting-sounding project names and winning competitions. After continuing the Insights tradition of winning at [BOOM]() in April with our sensationally-named "Fake News Detection" project (pictured above), I thought it would be nice to reflect a bit on the outcome of nearly a year of our work. This won't be a formal writeup that goes into a lot of technical details; instead, I will talk about my main takeaways from the project.

When we were doing new project pitches in fall 2018, many of the team members wanted to work on a project related to fake news (since that was on everyone's minds and seemed like a hot topic to investigate). Immediately, we ran into a few probems - how would we even define fake news? There's a lot of different definitions floating around, and many different ways for news articles to be misleading or inaccurate. To narrow the scope of the project, we decided to use an existing framework - the [Fake News Challenge](). 

FNC was a 2017 competition that essentially boiled down to detecting the relevance and stance (_agree_, _disagree_, _discuss_/_no opinion_) of an article compared to a particular claim. Instead of focusing on checking facts, we aimed to create a model that could flag articles that agreed with obviously false claims like "vaccines cause autism" or "Obama is Muslim and was born in Kenya"; the flagged articles could be passed onto human fact-checkers, thus allowing them focus their efforts on articles that are more likely to be fake news.

Although the competition had already ended, we decided to use the same basic problem description, dataset, and evaluation metrics because we felt that the competition's task was well-defined and it would be useful to compare our model with the winners of the competition to see how we did. However, not everything would be the same. Since we were a visualization-focused team, we added an additional goal of making our models and results easily interpretable. Since fake news was such a sensitive topic and state-of-the-art models are often viewed as black boxes, we thought that increasing transparency and interpretability would be key to gaining the trust of potential users of the system.

The first semester's work mostly focused on relevance detection, and building up a preprocessing codebase that was modular enough so we could change our features/models easily. We found that using hand-crafted features mostly derived from bag-of-words was very effective when input into a simple model (accuracy of ~95%). This was a relief, albeit a small one since relevance detection was expected to be the easier of the two tasks. Our visualization for that semester was a visualization of [random forests](), showing how frequently each rule was used and how pure each partition was. It was useful as a diagnostic tool, but not very useful for the end-user.

The second semester, we focused on the stance-detection task. Building off of experimentation with CNN's and LSTM's our previous semester, we tried more advanced architectures like siamese networks that were more suited to the two-part input (claim and article). Ultimately, we were only able to score a few percentage points above the baseline despite experimenting with several different architectures and embeddings. Midway through the semester, we switched gears and looked at a different approach which relied on handcrafted features based on dependency trees and simpler models. We felt that using dependency tree features in our model would be easier to visualize and interpret than a neural net. In the end, this approach scored similarly to the neural nets. 

On the visual side, we developed a much more cohesive visualization that overlaid our features onto both the source text and dependency trees of selected sentences. Although it could be refined further, we felt that the visualizations we created could be understood by untrained users relatively quickly. As a proof-of-concept and for demo purposes, we connected the visualization frontend to a server which used our model to classify and return the results and the preprocessed features that the model used.

From the technical side, we struggled a lot with the stance detection task. One reason was that the dataset had a really high baseline - it was extremely unbalanced, with the more interesting _agree_ and _disagree_ labels accounting for only 8% of the dataset combined (see below). To address this, we created a separate training set for stance detection which excluded articles that were unrelated to the claim. This new training set was slightly more balanced but still not good enough.

![dataset label distributions]()

Due to the label imbalance in the dataset, oversampling and subsampling failed to produce major increases in overall score. Large increases in accuracy for predicting the _agree_/_disagree_ labels were almost entirely offset by the small decrease in accuracy for predicting the _discuss_ label. In the end, we chose in favor of better accuracy at predicting the _agree_/_disagree_ classes, since they were more interesting. 

Although the dependency tree model performed similarly to the siamese network, by processing each sentence in the article separately we lost the ability to model relationships across sentences which was naturally present in LSTM's. This is important because news articles may include polarized quotes for the purposes of objective discussion or even to refute them; thus, there's no guarantee that every sentence in the article is representative of the stance of the article as a whole. My hunch is that our score would go up a lot if we found a good way to model the relationship between sentences.

The lack of deep-learning expertise on the team was a big problem througout this project. The subteams of CDS work almost entirely separately, and most of the deep-learning experts were on the Intelligent Systems subteam since that was their focus. Thanks to frequent consultations with IS subteam members throughout the second semester, this project ended up being very valuable for increasing the capability of our team to implement and use neural nets in future projects. 

Although I picked up some Pytorch during the course of this project, I can't say I'm a huge fan of neural networks. The benefits of using neural networks are higher generalizability and less time spent doing feature engineering, while the main drawback is a lack of interpretability. In the current tech environment, trustworthiness and transparency is a major issue. This means that there are significant advantages in using a model that can explain how and why it gave a certain output. In most situations, neural networks don't offer a significant performance improvement over simpler models, so if I had the time to do feature engineering I would prefer the latter. 

The high turnover on this team between semesters is a bit concerning - I was the only member of the team that decided to stay on the project for a second semester. For the first semester, I wanted to make sure repeating the mistakes of the previous semester and took a more active role in managing and delegating tasks for the project. I'm not sure how much of the turnover can be attributed to mistakes I made there - I'm on good terms with everyone on the team, and some of the members that left mentioned not being interested in building neural networks (which was the planned focus for the second semester). However, I probably could have delegated tasks better so that each member could explore areas that they were interested in, instead of suggesting tasks based entirely on what the project needed. Regardless, I think it's a good thing for anyone in a leadership role to be more aware and accomodating of each team member's interests and strengths.

Overall, I felt that this project went a lot more smoothly than the last. It remains to be seen whether the Fake News project will continue for another semester. Although there's still a lot of room for improvement in both our model and our visualization, I would be okay if we moved on to explore different topics. Since we aren't a research group, it's relatively difficult to maintain projects for more than two semesters, and I'm happy with the progress that we've made so far.

